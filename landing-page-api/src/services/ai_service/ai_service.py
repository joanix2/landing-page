"""Service IA pour la pr√©-compl√©tion du formulaire d'estimation."""

import os
import hashlib
from typing import Optional
from datetime import datetime, timedelta
from pathlib import Path
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from jinja2 import Environment, FileSystemLoader


class EstimationSuggestion(BaseModel):
    """Mod√®le pour les suggestions d'estimation (sortie IA)."""
    
    type_projet: str = Field(
        description="Type de projet sugg√©r√© parmi: Landing Page, Site Vitrine, E-commerce, Projet Sur Mesure"
    )
    liste_pages: list[str] = Field(
        description="Liste compl√®te des pages n√©cessaires pour le projet (ex: ['Accueil', '√Ä propos', 'Contact'])"
    )
    explication: str = Field(
        description="Br√®ve explication du projet et des pages sugg√©r√©es (2-3 phrases)"
    )


class AIService:
    """Service pour l'assistance IA lors du remplissage du formulaire."""
    
    def __init__(self):
        """Initialiser le service IA avec OpenAI."""
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY n'est pas d√©finie dans les variables d'environnement")
        
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.3,
            api_key=api_key
        )
        
        # Parser pour structurer la sortie
        self.parser = PydanticOutputParser(pydantic_object=EstimationSuggestion)
        
        # Configuration de Jinja2 pour les templates de prompts
        template_dir = Path(__file__).parent / "templates"
        self.jinja_env = Environment(loader=FileSystemLoader(str(template_dir)))
        
        # Charger les templates de prompts
        self.system_prompt_template = self.jinja_env.get_template("system_prompt.txt.j2")
        self.user_prompt_template = self.jinja_env.get_template("user_prompt.txt.j2")
    
    def _render_prompts(self, description: str, format_instructions: str) -> tuple[str, str]:
        """
        Rendre les templates de prompts avec Jinja2.
        
        Args:
            description: Description du projet
            format_instructions: Instructions de formatage
            
        Returns:
            Tuple (system_prompt, user_prompt)
        """
        system_prompt = self.system_prompt_template.render(
            format_instructions=format_instructions
        )
        user_prompt = self.user_prompt_template.render(
            description=description
        )
        return system_prompt, user_prompt
    
    def _calculate_budget(self, nombre_pages: int) -> str:
        """
        Calculer le budget en fonction du nombre de pages.
        Formule : nombre_pages √ó 3 √ó 500‚Ç¨
        """
        montant = nombre_pages * 3 * 500
        
        if montant < 5000:
            return "Moins de 5 000‚Ç¨"
        elif montant <= 10000:
            return "5 000‚Ç¨ - 10 000‚Ç¨"
        elif montant <= 20000:
            return "10 000‚Ç¨ - 20 000‚Ç¨"
        else:
            return "Plus de 20 000‚Ç¨"
    
    def _calculate_delai(self, nombre_pages: int) -> str:
        """
        Calculer le d√©lai en fonction du nombre de pages.
        Formule : nombre_pages √ó 3 jours
        """
        jours = nombre_pages * 3
        
        if jours <= 15: # Moins de deux semaines
            return "Rapide"
        elif jours <= 60:  # 1-2 mois
            return "Normal"
        else:  # Plus de 2 mois
            return "Flexible"
    
    @staticmethod
    def _hash_description(description: str) -> str:
        """G√©n√©rer un hash SHA256 de la description pour le cache."""
        normalized = description.lower().strip()
        return hashlib.sha256(normalized.encode()).hexdigest()
    
    async def _get_from_cache(
        self, 
        description: str, 
        db: AsyncSession
    ) -> Optional[EstimationSuggestion]:
        """R√©cup√©rer une suggestion depuis le cache."""
        from src.models.ai_cache import AISuggestionCache
        
        description_hash = self._hash_description(description)
        
        result = await db.execute(
            select(AISuggestionCache).where(
                AISuggestionCache.description_hash == description_hash
            )
        )
        cached = result.scalar_one_or_none()
        
        if cached:
            # Mettre √† jour les statistiques d'utilisation
            cached.used_count += 1
            cached.last_used_at = datetime.utcnow()
            await db.commit()
            
            print(f"‚úÖ Suggestion trouv√©e dans le cache (utilis√©e {cached.used_count} fois)")
            
            return EstimationSuggestion(
                type_projet=cached.type_projet,
                liste_pages=cached.liste_pages,
                explication=cached.explication
            )
        
        return None
    
    async def _save_to_cache(
        self,
        description: str,
        suggestion: EstimationSuggestion,
        db: AsyncSession
    ) -> None:
        """Sauvegarder une suggestion dans le cache."""
        from src.models.ai_cache import AISuggestionCache
        
        description_hash = self._hash_description(description)
        
        # V√©rifier si existe d√©j√† (race condition)
        result = await db.execute(
            select(AISuggestionCache).where(
                AISuggestionCache.description_hash == description_hash
            )
        )
        existing = result.scalar_one_or_none()
        
        if not existing:
            cached = AISuggestionCache(
                description_hash=description_hash,
                description_projet=description,
                type_projet=suggestion.type_projet,
                liste_pages=suggestion.liste_pages,
                explication=suggestion.explication
            )
            db.add(cached)
            await db.commit()
            print("üíæ Suggestion sauvegard√©e dans le cache")
    
    async def suggest_estimation_params(
        self, 
        description_projet: str,
        db: Optional[AsyncSession] = None
    ) -> Optional[EstimationSuggestion]:
        """
        Analyser la description du projet et sugg√©rer des param√®tres.
        Utilise le cache PostgreSQL si disponible.
        
        Args:
            description_projet: Description textuelle du projet client
            db: Session de base de donn√©es (optionnelle, pour le cache)
            
        Returns:
            EstimationSuggestion avec les param√®tres sugg√©r√©s ou None en cas d'erreur
        """
        try:
            # V√©rifier le cache si DB disponible
            if db:
                cached_suggestion = await self._get_from_cache(description_projet, db)
                if cached_suggestion:
                    return cached_suggestion
            
            # Pas de cache, appeler l'IA
            print("ü§ñ G√©n√©ration de nouvelles suggestions via IA...")
            
            # Rendre les prompts avec Jinja2
            system_prompt, user_prompt = self._render_prompts(
                description_projet,
                self.parser.get_format_instructions()
            )
            
            # Cr√©er le prompt LangChain
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("user", user_prompt)
            ])
            
            # Ex√©cuter la cha√Æne
            chain = prompt | self.llm | self.parser
            result = await chain.ainvoke({})
            
            # Sauvegarder dans le cache si DB disponible
            if db and result:
                await self._save_to_cache(description_projet, result, db)
            
            return result
            
        except Exception as e:
            print(f"Erreur lors de la suggestion IA : {e}")
            return None
    
    async def analyze_and_suggest(
        self, 
        description_projet: str,
        db: Optional[AsyncSession] = None
    ) -> dict:
        """
        Analyser le projet et retourner des suggestions format√©es.
        Utilise le cache PostgreSQL si disponible.
        
        Args:
            description_projet: Description du projet
            db: Session de base de donn√©es (optionnelle, pour le cache)
            
        Returns:
            Dictionnaire avec les suggestions ou un message d'erreur
        """
        if not description_projet or len(description_projet.strip()) < 20:
            return {
                "success": False,
                "message": "La description du projet doit contenir au moins 20 caract√®res pour obtenir des suggestions pertinentes."
            }
        
        suggestion = await self.suggest_estimation_params(description_projet, db)
        
        if not suggestion:
            return {
                "success": False,
                "message": "Impossible de g√©n√©rer des suggestions pour le moment. Veuillez r√©essayer."
            }
        
        # Calculer les valeurs d√©riv√©es √† partir de liste_pages
        nombre_pages = len(suggestion.liste_pages)
        budget = self._calculate_budget(nombre_pages)
        delai_souhaite = self._calculate_delai(nombre_pages)
        
        return {
            "success": True,
            "suggestions": {
                "type_projet": suggestion.type_projet,
                "liste_pages": suggestion.liste_pages,
                "nombre_pages": nombre_pages,
                "delai_souhaite": delai_souhaite,
                "budget": budget
            },
            "explication": suggestion.explication,
            "from_cache": db is not None  # Indiquer si provient du cache
        }


# Instance singleton du service
_ai_service: Optional[AIService] = None


def get_ai_service() -> AIService:
    """Obtenir l'instance du service IA (singleton)."""
    global _ai_service
    if _ai_service is None:
        _ai_service = AIService()
    return _ai_service
